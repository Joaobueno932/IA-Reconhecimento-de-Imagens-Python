{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1xM9cTBBDFVvjMp6IrPfsPjRX7OXHulLn","timestamp":1683729596036}],"authorship_tag":"ABX9TyMxZ8m1L867F+qNtCKMIC0G"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"CaFuQMoh67K4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684001160277,"user_tz":180,"elapsed":2227,"user":{"displayName":"Kevin Nicolas Costantino","userId":"03967773680110882059"}},"outputId":"12098cc2-6cbc-4444-c0ee-f7ef19bd3d39"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["from google.colab import files\n","uploaded = files.upload()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":38},"id":"ioOVrxLnZ448","executionInfo":{"status":"ok","timestamp":1684001176351,"user_tz":180,"elapsed":16086,"user":{"displayName":"Kevin Nicolas Costantino","userId":"03967773680110882059"}},"outputId":"8962d8cf-7118-477a-e752-82ef4dd0ab14"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-b4d0c268-fd8a-4410-bf59-7c70f6e0f88b\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-b4d0c268-fd8a-4410-bf59-7c70f6e0f88b\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}}]},{"cell_type":"code","source":["print (uploaded['OrigaList.csv'][:200].decode('utf-8') + '...')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zPZIbz0aaJcS","executionInfo":{"status":"ok","timestamp":1683923709601,"user_tz":180,"elapsed":266,"user":{"displayName":"Kevin Nicolas Costantino","userId":"03967773680110882059"}},"outputId":"f7f7a2ab-6ea0-4215-cb83-06f05aec5496"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Eye,Filename,ExpCDR,Set,Glaucoma\r\n","OD,001.jpg,0.7097,A,0\r\n","OS,002.jpg,0.6953,A,0\r\n","OS,003.jpg,0.9629,A,0\r\n","OD,004.jpg,0.7246,A,0\r\n","OS,005.jpg,0.6138,A,0\r\n","OD,006.jpg,0.6451,B,0\r\n","OS,007.jpg,0.5255,A,0\r\n","OD,00...\n"]}]},{"cell_type":"markdown","source":["Passa arquivos de uma pasta do drive pra outra, usada pra passar as imagens de um banco de dados pra outra pasta"],"metadata":{"id":"oeiIlQiYMR6e"}},{"cell_type":"code","source":["import os\n","import csv\n","import shutil\n","from PIL import Image\n","\n","def convert_image(file_path, destination, target_extension):\n","    image = Image.open(file_path)\n","    image = image.convert(\"RGB\")\n","    new_file_path = os.path.splitext(destination)[0] + \".\" + target_extension\n","    image.save(new_file_path)\n","    shutil.move(file_path, new_file_path)\n","\n","out = open(\"OrigaList.csv\", \"r\")\n","data = csv.reader(out)\n","data = [[row[0], row[1], row[2], row[3], row[4]] for row in data] \n","class1 = []\n","\n","for row in data:\n","    if row[4] == '1':\n","        aa = row[1]\n","        class1.append(aa)\n","        ee = \"/content/drive/My Drive/compara_segmentadores/data2/A/ORIGA/Masks/\" + aa\n","        destination = \"/content/drive/My Drive/compara_segmentadores/data2/A/ORIGA/classe 1/\" + aa.replace(\".png\", \".jpg\")\n","        convert_image(ee, destination, \"jpg\")\n","    elif row[4] == '0':\n","        bb = row[1]\n","        class1.append(bb)\n","        gg = \"/content/drive/My Drive/compara_segmentadores/data2/A/ORIGA/Masks/\" + bb\n","        destination = \"/content/drive/My Drive/compara_segmentadores/data2/A/ORIGA/classe 0/\" + bb.replace(\".png\", \".jpg\")\n","        convert_image(gg, destination, \"jpg\")\n","\n","out.close()\n","print(data)\n","new_data = [[row[0], row[1], row[2], row[3], row[4]] for row in data]\n","\n","out = open(\"new_data.csv\", \"w\")\n","output = csv.writer(out)\n","for row in new_data:\n","    output.writerow(row)\n","out.close()\n"],"metadata":{"id":"3nMdIaa5f_Yl","colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"status":"error","timestamp":1684579176431,"user_tz":180,"elapsed":462,"user":{"displayName":"Kevin Nicolas Costantino","userId":"03967773680110882059"}},"outputId":"ed73d761-88f3-4ece-c7a5-fd8c0cbfc551"},"execution_count":2,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-6703c02a8ac2>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"OrigaList.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'OrigaList.csv'"]}]},{"cell_type":"code","source":["import os \n","print(os.listdir((\"/content/drive/My Drive/compara_segmentadores/data2/A/ORIGA\")))"],"metadata":{"id":"gSQ55M5Q8YZi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683923722013,"user_tz":180,"elapsed":6,"user":{"displayName":"Kevin Nicolas Costantino","userId":"03967773680110882059"}},"outputId":"5d59e101-5f4c-4aca-96b9-6c9cefa517d4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["['OrigaList.csv', 'origa_info.csv', 'Images_Cropped', 'Images_Square', 'Images', 'Masks_Square', 'Masks', 'Masks_Cropped', 'Semi-automatic-annotations', 'Class1', 'Class0', 'train', 'test']\n"]}]},{"cell_type":"code","source":["! pip install -q kaggle\n","\n","from google.colab import files\n","\n","files.upload()\n","\n","! mkdir ~/.kaggle\n","\n","! cp kaggle.json ~/.kaggle/\n","\n","! chmod 600 ~/.kaggle/kaggle.json\n","\n","! kaggle datasets list"],"metadata":{"id":"uLxSHcOmQLnX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.models import Sequential\n","from keras.layers import Conv2D\n","from keras.layers import MaxPooling2D\n","from keras.layers import Flatten\n","from keras.layers import Dense\n","import keras.utils as image\n","from keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","\n","#import os\n","#for dirname, _, filenames in os.walk('/kaggle/input/glaucoma-datasets'):\n","#    for filename in filenames:\n","#        print(os.path.join(dirname, filename))\n","        \n","\n","\n","\n","# Initialising the CNN\n","classifier = Sequential()\n","# Step 1 - Convolution\n","classifier.add(Conv2D(32, (3, 3), input_shape = (256,256, 3), activation = 'relu'))\n","# Step 2 - Pooling\n","classifier.add(MaxPooling2D(pool_size = (2, 2)))\n","# Adding a second convolutional layer\n","classifier.add(Conv2D(32, (3, 3), activation = 'relu'))\n","classifier.add(MaxPooling2D(pool_size = (2, 2)))\n","# Step 3 - Flattening\n","classifier.add(Flatten())\n","# Step 4 - Full connection\n","classifier.add(Dense(units = 128, activation = 'relu'))\n","classifier.add(Dense(units = 1, activation = 'sigmoid'))\n","# Compiling the CNN\n","classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"],"metadata":{"id":"ZaVhrrKp8dbf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Callbacks\n","checkpoint = ModelCheckpoint(\"f1.h5\", monitor='acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', save_freq='epoch')\n","reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.1, patience=2, verbose=0, mode='auto', cooldown=0, min_lr=0)"],"metadata":{"id":"eAV2OjNIm3--"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from keras.preprocessing.image import ImageDataGenerator\n","train_datagen = ImageDataGenerator(rescale = 1./255,\n","shear_range = 0.2,\n","zoom_range = 0.2,\n","horizontal_flip = True)\n","# print(train_datagen)\n","test_datagen = ImageDataGenerator(rescale = 1./255)\n","training_set = train_datagen.flow_from_directory('/content/drive/My Drive/compara_segmentadores/data2/A/ORIGA/train/',\n","target_size = (256,256),\n","batch_size = 32,\n","class_mode = 'binary')\n","# print(test_datagen)\n","test_set = test_datagen.flow_from_directory('/content/drive/My Drive/compara_segmentadores/data2/A/ORIGA/test/',\n","target_size = (256,256),\n","batch_size = 32,\n","class_mode = 'binary')"],"metadata":{"id":"voW9DLIJ8d8Q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1684001183747,"user_tz":180,"elapsed":9,"user":{"displayName":"Kevin Nicolas Costantino","userId":"03967773680110882059"}},"outputId":"1134bc8a-af33-4bb4-8578-1009758eb18c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 603 images belonging to 2 classes.\n","Found 47 images belonging to 2 classes.\n"]}]},{"cell_type":"code","source":["\n","!apt-get install python3.6\n","\n"],"metadata":{"id":"IP7DIt16OVK0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install tensorflow==2.8.0\n"],"metadata":{"id":"RlFPRjdGPpvF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size=32\n","model_info=classifier.fit_generator(training_set,\n","steps_per_epoch = 455//batch_size,\n","epochs = 30,\n","validation_data = test_set,\n","validation_steps = 30//batch_size,callbacks=[checkpoint, reduce_lr])"],"metadata":{"id":"kYvZW1mg8isf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install --upgrade tensorflow"],"metadata":{"id":"VNolHVt-0ylg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n","\n","# Definir o tamanho das imagens de entrada\n","img_size = (224, 224)\n","\n","# Definir o tamanho do batch e número de épocas\n","batch_size = 32\n","epochs = 100\n","\n","# Definir o número de passos por época para o conjunto de treinamento e validação\n","train_steps_per_epoch = 455 // batch_size\n","val_steps_per_epoch = 30 // batch_size\n","\n","# Definir o número de filtros para cada camada convolucional\n","num_filters = [32, 64, 128]\n","\n","# Definir a taxa de aprendizagem inicial\n","learning_rate = 1e-4\n","\n","# Criar geradores de imagens para treinamento e validação\n","train_datagen = ImageDataGenerator(\n","    rescale=1./255,      # normalizar os valores dos pixels\n","    rotation_range=20,   # girar aleatoriamente as imagens em até 20 graus\n","    width_shift_range=0.2,   # mover aleatoriamente as imagens horizontalmente\n","    height_shift_range=0.2,  # mover aleatoriamente as imagens verticalmente\n","    shear_range=0.2,     # aplicar cisalhamento nas imagens\n","    zoom_range=0.2,      # aplicar zoom nas imagens\n","    horizontal_flip=True,    # espelhar aleatoriamente as imagens horizontalmente\n","    validation_split=0.2   # definir a proporção de imagens para validação\n",")\n","\n","# Criar gerador de imagens de treinamento\n","train_generator = train_datagen.flow_from_directory(\n","    '/content/drive/My Drive/compara_segmentadores/data2/A/ORIGA/train/',  # diretório com as imagens\n","    target_size=img_size,    # tamanho das imagens\n","    batch_size=batch_size,     # tamanho do lote\n","    class_mode='binary',       # modo de classificação (binário ou multiclasse)\n","    subset='training',         # usar apenas as imagens de treinamento\n","    shuffle=True               # embaralhar as imagens a cada época\n",")\n","\n","# Criar gerador de imagens de validação\n","val_generator = train_datagen.flow_from_directory(\n","    '/content/drive/My Drive/compara_segmentadores/data2/A/ORIGA/valid/',  # diretório com as imagens\n","    target_size=img_size,    # tamanho das imagens\n","    batch_size=batch_size,     # tamanho do lote\n","    class_mode='binary',       # modo de classificação (binário ou multiclasse)\n","    subset='validation',       # usar apenas as imagens de validação\n","    shuffle=True               # embaralhar as imagens a cada época\n",")\n","\n","val_datagen = ImageDataGenerator(rescale=1./255)\n","\n","# Carregar os conjuntos de treinamento e validação\n","train_set = train_datagen.flow_from_directory(\n","    directory='/content/drive/My Drive/compara_segmentadores/data2/A/ORIGA/train/',\n","    target_size=img_size,\n","    batch_size=batch_size,\n","    class_mode='binary'\n",")\n","\n","\n","\n","val_set = val_datagen.flow_from_directory(\n","    directory='/content/drive/My Drive/compara_segmentadores/data2/A/ORIGA/valid/',\n","    target_size=img_size,\n","    batch_size=batch_size,\n","    class_mode='binary'\n",")\n","\n","# Definir o modelo da rede neural\n","model = tf.keras.models.Sequential([\n","    # Camada convolucional 1\n","    tf.keras.layers.Conv2D(num_filters[0], 3, activation='relu', input_shape=(img_size[0], img_size[1], 3)),\n","    tf.keras.layers.MaxPooling2D(2),\n","    # Camada convolucional 2\n","    tf.keras.layers.Conv2D(num_filters[1], 3, activation='relu'),\n","    tf.keras.layers.MaxPooling2D(2),\n","    # Camada convolucional 3\n","    tf.keras.layers.Conv2D(num_filters[2], 3, activation='relu'),\n","    tf.keras.layers.MaxPooling2D(2),\n","    # Camada Flatten\n","    tf.keras.layers.Flatten(),\n","    # Camada Dense\n","    tf.keras.layers.Dense(128, activation='relu'),\n","    tf.keras.layers.Dropout(0.5),\n","    # Camada de saída\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])\n","\n","# Compilar o modelo\n","model.compile(\n","    loss='binary_crossentropy',\n","    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n","    metrics=['accuracy']\n",")\n","\n","# Definir os callbacks\n","checkpoint = ModelCheckpoint('path/to/model.h5', monitor='val_accuracy', save_best_only=True, mode='max', verbose=1,save_freq='epoch')\n","reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-7, mode='min', verbose=1)\n","\n","# Treinar o modelo\n","model_info = model.fit(\n","    train_set,\n","    epochs=epochs,\n","    validation_data=val_set,\n","    steps_per_epoch=train_steps_per_epoch,\n","    validation_steps=val_steps_per_epoch,\n","    callbacks=[checkpoint, reduce_lr]\n",")\n","\n","\n"],"metadata":{"id":"lJBdXFOXxv8j","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1684022534295,"user_tz":180,"elapsed":6464627,"user":{"displayName":"Kevin Nicolas Costantino","userId":"03967773680110882059"}},"outputId":"9a5caef1-e697-4920-8abb-f4aa716f331d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 470 images belonging to 2 classes.\n","Found 4 images belonging to 2 classes.\n","Found 587 images belonging to 2 classes.\n","Found 23 images belonging to 2 classes.\n","Epoch 1/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5713 - accuracy: 0.7588"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 65s 4s/step - loss: 0.5713 - accuracy: 0.7588 - lr: 1.0000e-04\n","Epoch 2/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5593 - accuracy: 0.7541"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 61s 4s/step - loss: 0.5593 - accuracy: 0.7541 - lr: 1.0000e-04\n","Epoch 3/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5761 - accuracy: 0.7455"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 64s 4s/step - loss: 0.5761 - accuracy: 0.7455 - lr: 1.0000e-04\n","Epoch 4/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5691 - accuracy: 0.7589"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 64s 4s/step - loss: 0.5691 - accuracy: 0.7589 - lr: 1.0000e-04\n","Epoch 5/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5404 - accuracy: 0.7752"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 61s 4s/step - loss: 0.5404 - accuracy: 0.7752 - lr: 1.0000e-04\n","Epoch 6/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5436 - accuracy: 0.7634"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 64s 5s/step - loss: 0.5436 - accuracy: 0.7634 - lr: 1.0000e-04\n","Epoch 7/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5200 - accuracy: 0.7775"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 61s 4s/step - loss: 0.5200 - accuracy: 0.7775 - lr: 1.0000e-04\n","Epoch 8/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5827 - accuracy: 0.7494"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 60s 4s/step - loss: 0.5827 - accuracy: 0.7494 - lr: 1.0000e-04\n","Epoch 9/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5748 - accuracy: 0.7447"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 62s 5s/step - loss: 0.5748 - accuracy: 0.7447 - lr: 1.0000e-04\n","Epoch 10/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5460 - accuracy: 0.7658"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 62s 4s/step - loss: 0.5460 - accuracy: 0.7658 - lr: 1.0000e-04\n","Epoch 11/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5406 - accuracy: 0.7752"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 61s 5s/step - loss: 0.5406 - accuracy: 0.7752 - lr: 1.0000e-04\n","Epoch 12/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5631 - accuracy: 0.7567"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 64s 4s/step - loss: 0.5631 - accuracy: 0.7567 - lr: 1.0000e-04\n","Epoch 13/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5289 - accuracy: 0.7799"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 60s 4s/step - loss: 0.5289 - accuracy: 0.7799 - lr: 1.0000e-04\n","Epoch 14/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5444 - accuracy: 0.7723"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 63s 4s/step - loss: 0.5444 - accuracy: 0.7723 - lr: 1.0000e-04\n","Epoch 15/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5513 - accuracy: 0.7658"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 59s 4s/step - loss: 0.5513 - accuracy: 0.7658 - lr: 1.0000e-04\n","Epoch 16/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5370 - accuracy: 0.7775"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 61s 5s/step - loss: 0.5370 - accuracy: 0.7775 - lr: 1.0000e-04\n","Epoch 17/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5513 - accuracy: 0.7567"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 64s 4s/step - loss: 0.5513 - accuracy: 0.7567 - lr: 1.0000e-04\n","Epoch 18/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5548 - accuracy: 0.7588"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 61s 5s/step - loss: 0.5548 - accuracy: 0.7588 - lr: 1.0000e-04\n","Epoch 19/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5481 - accuracy: 0.7635"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 62s 4s/step - loss: 0.5481 - accuracy: 0.7635 - lr: 1.0000e-04\n","Epoch 20/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5803 - accuracy: 0.7424"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 61s 4s/step - loss: 0.5803 - accuracy: 0.7424 - lr: 1.0000e-04\n","Epoch 21/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5681 - accuracy: 0.7494"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 60s 4s/step - loss: 0.5681 - accuracy: 0.7494 - lr: 1.0000e-04\n","Epoch 22/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5776 - accuracy: 0.7377"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 61s 4s/step - loss: 0.5776 - accuracy: 0.7377 - lr: 1.0000e-04\n","Epoch 23/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5696 - accuracy: 0.7500"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 64s 4s/step - loss: 0.5696 - accuracy: 0.7500 - lr: 1.0000e-04\n","Epoch 24/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5715 - accuracy: 0.7447"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 60s 4s/step - loss: 0.5715 - accuracy: 0.7447 - lr: 1.0000e-04\n","Epoch 25/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5587 - accuracy: 0.7455"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 64s 4s/step - loss: 0.5587 - accuracy: 0.7455 - lr: 1.0000e-04\n","Epoch 26/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5232 - accuracy: 0.7681"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 61s 4s/step - loss: 0.5232 - accuracy: 0.7681 - lr: 1.0000e-04\n","Epoch 27/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5528 - accuracy: 0.7611"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 62s 4s/step - loss: 0.5528 - accuracy: 0.7611 - lr: 1.0000e-04\n","Epoch 28/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5305 - accuracy: 0.7812"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 65s 5s/step - loss: 0.5305 - accuracy: 0.7812 - lr: 1.0000e-04\n","Epoch 29/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5604 - accuracy: 0.7518"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 62s 4s/step - loss: 0.5604 - accuracy: 0.7518 - lr: 1.0000e-04\n","Epoch 30/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5264 - accuracy: 0.7822"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 61s 4s/step - loss: 0.5264 - accuracy: 0.7822 - lr: 1.0000e-04\n","Epoch 31/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5556 - accuracy: 0.7635"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 60s 4s/step - loss: 0.5556 - accuracy: 0.7635 - lr: 1.0000e-04\n","Epoch 32/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5610 - accuracy: 0.7447"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 61s 4s/step - loss: 0.5610 - accuracy: 0.7447 - lr: 1.0000e-04\n","Epoch 33/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5356 - accuracy: 0.7681"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 61s 4s/step - loss: 0.5356 - accuracy: 0.7681 - lr: 1.0000e-04\n","Epoch 34/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5609 - accuracy: 0.7564"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 60s 4s/step - loss: 0.5609 - accuracy: 0.7564 - lr: 1.0000e-04\n","Epoch 35/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5409 - accuracy: 0.7611"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 60s 4s/step - loss: 0.5409 - accuracy: 0.7611 - lr: 1.0000e-04\n","Epoch 36/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5592 - accuracy: 0.7447"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 62s 4s/step - loss: 0.5592 - accuracy: 0.7447 - lr: 1.0000e-04\n","Epoch 37/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5408 - accuracy: 0.7611"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 61s 4s/step - loss: 0.5408 - accuracy: 0.7611 - lr: 1.0000e-04\n","Epoch 38/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5488 - accuracy: 0.7611"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 61s 4s/step - loss: 0.5488 - accuracy: 0.7611 - lr: 1.0000e-04\n","Epoch 39/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5410 - accuracy: 0.7588"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 60s 4s/step - loss: 0.5410 - accuracy: 0.7588 - lr: 1.0000e-04\n","Epoch 40/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5443 - accuracy: 0.7611"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 61s 4s/step - loss: 0.5443 - accuracy: 0.7611 - lr: 1.0000e-04\n","Epoch 41/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5347 - accuracy: 0.7822"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 61s 4s/step - loss: 0.5347 - accuracy: 0.7822 - lr: 1.0000e-04\n","Epoch 42/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5690 - accuracy: 0.7567"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 62s 4s/step - loss: 0.5690 - accuracy: 0.7567 - lr: 1.0000e-04\n","Epoch 43/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5366 - accuracy: 0.7701"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 64s 5s/step - loss: 0.5366 - accuracy: 0.7701 - lr: 1.0000e-04\n","Epoch 44/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5372 - accuracy: 0.7681"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 62s 4s/step - loss: 0.5372 - accuracy: 0.7681 - lr: 1.0000e-04\n","Epoch 45/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5534 - accuracy: 0.7567"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 64s 5s/step - loss: 0.5534 - accuracy: 0.7567 - lr: 1.0000e-04\n","Epoch 46/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5440 - accuracy: 0.7658"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 61s 4s/step - loss: 0.5440 - accuracy: 0.7658 - lr: 1.0000e-04\n","Epoch 47/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5621 - accuracy: 0.7447"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 60s 4s/step - loss: 0.5621 - accuracy: 0.7447 - lr: 1.0000e-04\n","Epoch 48/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5252 - accuracy: 0.7822"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 62s 4s/step - loss: 0.5252 - accuracy: 0.7822 - lr: 1.0000e-04\n","Epoch 49/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5463 - accuracy: 0.7545"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 63s 4s/step - loss: 0.5463 - accuracy: 0.7545 - lr: 1.0000e-04\n","Epoch 50/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5497 - accuracy: 0.7522"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 64s 5s/step - loss: 0.5497 - accuracy: 0.7522 - lr: 1.0000e-04\n","Epoch 51/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5422 - accuracy: 0.7611"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 61s 4s/step - loss: 0.5422 - accuracy: 0.7611 - lr: 1.0000e-04\n","Epoch 52/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5650 - accuracy: 0.7500"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 63s 4s/step - loss: 0.5650 - accuracy: 0.7500 - lr: 1.0000e-04\n","Epoch 53/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5494 - accuracy: 0.7500"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 63s 4s/step - loss: 0.5494 - accuracy: 0.7500 - lr: 1.0000e-04\n","Epoch 54/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5243 - accuracy: 0.7681"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 61s 4s/step - loss: 0.5243 - accuracy: 0.7681 - lr: 1.0000e-04\n","Epoch 55/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5532 - accuracy: 0.7494"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 62s 4s/step - loss: 0.5532 - accuracy: 0.7494 - lr: 1.0000e-04\n","Epoch 56/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5480 - accuracy: 0.7541"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 60s 4s/step - loss: 0.5480 - accuracy: 0.7541 - lr: 1.0000e-04\n","Epoch 57/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5234 - accuracy: 0.7658"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 62s 4s/step - loss: 0.5234 - accuracy: 0.7658 - lr: 1.0000e-04\n","Epoch 58/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5347 - accuracy: 0.7494"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 61s 4s/step - loss: 0.5347 - accuracy: 0.7494 - lr: 1.0000e-04\n","Epoch 59/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5545 - accuracy: 0.7522"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 64s 4s/step - loss: 0.5545 - accuracy: 0.7522 - lr: 1.0000e-04\n","Epoch 60/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5532 - accuracy: 0.7500"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 64s 4s/step - loss: 0.5532 - accuracy: 0.7500 - lr: 1.0000e-04\n","Epoch 61/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5570 - accuracy: 0.7518"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 62s 4s/step - loss: 0.5570 - accuracy: 0.7518 - lr: 1.0000e-04\n","Epoch 62/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5276 - accuracy: 0.7728"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 60s 4s/step - loss: 0.5276 - accuracy: 0.7728 - lr: 1.0000e-04\n","Epoch 63/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5462 - accuracy: 0.7611"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 62s 4s/step - loss: 0.5462 - accuracy: 0.7611 - lr: 1.0000e-04\n","Epoch 64/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5669 - accuracy: 0.7400"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 60s 4s/step - loss: 0.5669 - accuracy: 0.7400 - lr: 1.0000e-04\n","Epoch 65/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5210 - accuracy: 0.7775"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 60s 4s/step - loss: 0.5210 - accuracy: 0.7775 - lr: 1.0000e-04\n","Epoch 66/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5248 - accuracy: 0.7705"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 61s 4s/step - loss: 0.5248 - accuracy: 0.7705 - lr: 1.0000e-04\n","Epoch 67/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5578 - accuracy: 0.7471"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 62s 4s/step - loss: 0.5578 - accuracy: 0.7471 - lr: 1.0000e-04\n","Epoch 68/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5352 - accuracy: 0.7564"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 61s 4s/step - loss: 0.5352 - accuracy: 0.7564 - lr: 1.0000e-04\n","Epoch 69/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5098 - accuracy: 0.7705"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 61s 4s/step - loss: 0.5098 - accuracy: 0.7705 - lr: 1.0000e-04\n","Epoch 70/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5106 - accuracy: 0.7728"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 61s 4s/step - loss: 0.5106 - accuracy: 0.7728 - lr: 1.0000e-04\n","Epoch 71/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5243 - accuracy: 0.7518"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 61s 4s/step - loss: 0.5243 - accuracy: 0.7518 - lr: 1.0000e-04\n","Epoch 72/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5097 - accuracy: 0.7705"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 59s 4s/step - loss: 0.5097 - accuracy: 0.7705 - lr: 1.0000e-04\n","Epoch 73/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5395 - accuracy: 0.7588"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 62s 4s/step - loss: 0.5395 - accuracy: 0.7588 - lr: 1.0000e-04\n","Epoch 74/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5391 - accuracy: 0.7500"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 64s 4s/step - loss: 0.5391 - accuracy: 0.7500 - lr: 1.0000e-04\n","Epoch 75/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5152 - accuracy: 0.7658"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 61s 4s/step - loss: 0.5152 - accuracy: 0.7658 - lr: 1.0000e-04\n","Epoch 76/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5607 - accuracy: 0.7471"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 62s 4s/step - loss: 0.5607 - accuracy: 0.7471 - lr: 1.0000e-04\n","Epoch 77/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5305 - accuracy: 0.7681"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 62s 4s/step - loss: 0.5305 - accuracy: 0.7681 - lr: 1.0000e-04\n","Epoch 78/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5238 - accuracy: 0.7635"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 60s 4s/step - loss: 0.5238 - accuracy: 0.7635 - lr: 1.0000e-04\n","Epoch 79/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5249 - accuracy: 0.7658"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 61s 4s/step - loss: 0.5249 - accuracy: 0.7658 - lr: 1.0000e-04\n","Epoch 80/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5164 - accuracy: 0.7589"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 63s 4s/step - loss: 0.5164 - accuracy: 0.7589 - lr: 1.0000e-04\n","Epoch 81/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5185 - accuracy: 0.7612"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 64s 4s/step - loss: 0.5185 - accuracy: 0.7612 - lr: 1.0000e-04\n","Epoch 82/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5025 - accuracy: 0.7746"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 63s 4s/step - loss: 0.5025 - accuracy: 0.7746 - lr: 1.0000e-04\n","Epoch 83/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5291 - accuracy: 0.7567"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 70s 5s/step - loss: 0.5291 - accuracy: 0.7567 - lr: 1.0000e-04\n","Epoch 84/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5300 - accuracy: 0.7564"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 61s 4s/step - loss: 0.5300 - accuracy: 0.7564 - lr: 1.0000e-04\n","Epoch 85/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5420 - accuracy: 0.7424"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 62s 4s/step - loss: 0.5420 - accuracy: 0.7424 - lr: 1.0000e-04\n","Epoch 86/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5112 - accuracy: 0.7658"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 60s 4s/step - loss: 0.5112 - accuracy: 0.7658 - lr: 1.0000e-04\n","Epoch 87/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5207 - accuracy: 0.7681"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 59s 4s/step - loss: 0.5207 - accuracy: 0.7681 - lr: 1.0000e-04\n","Epoch 88/100\n","14/14 [==============================] - ETA: 0s - loss: 0.5110 - accuracy: 0.7746"]},{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:Can save best model only with val_accuracy available, skipping.\n","WARNING:tensorflow:Learning rate reduction is conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy,lr\n"]},{"output_type":"stream","name":"stdout","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r14/14 [==============================] - 64s 5s/step - loss: 0.5110 - accuracy: 0.7746 - lr: 1.0000e-04\n","Epoch 89/100\n","12/14 [========================>.....] - ETA: 8s - loss: 0.5469 - accuracy: 0.7190 "]},{"output_type":"error","ename":"UnknownError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-28-fc99a512912e>\u001b[0m in \u001b[0;36m<cell line: 105>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;31m# Treinar o modelo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m model_info = model.fit(\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     53\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mUnknownError\u001b[0m: Graph execution error:\n\nFileNotFoundError: [Errno 2] No such file or directory: '/content/drive/My Drive/compara_segmentadores/data2/A/ORIGA/train/classe 0 /361.jpg'\nTraceback (most recent call last):\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/script_ops.py\", line 267, in __call__\n    ret = func(*args)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/autograph/impl/api.py\", line 642, in wrapper\n    return func(*args, **kwargs)\n\n  File \"/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/engine/data_adapter.py\", line 902, in wrapped_generator\n    for data in generator_fn():\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/engine/data_adapter.py\", line 1049, in generator_fn\n    yield x[i]\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/preprocessing/image.py\", line 116, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/preprocessing/image.py\", line 370, in _get_batches_of_transformed_samples\n    img = image_utils.load_img(\n\n  File \"/usr/local/lib/python3.10/dist-packages/keras/utils/image_utils.py\", line 422, in load_img\n    with open(path, \"rb\") as f:\n\nFileNotFoundError: [Errno 2] No such file or directory: '/content/drive/My Drive/compara_segmentadores/data2/A/ORIGA/train/classe 0 /361.jpg'\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_train_function_21996]"]}]},{"cell_type":"code","source":["### Performance evaluation\n","#########################\n","score = model.evaluate_generator(val_set,accuracy)\n","print(\" Total: \", len(val_set.filenames))\n","print(\"Loss: \", score[0], \"Accuracy: \", score[1])\n","#print(\"Accuracy = \",score[1])"],"metadata":{"id":"yAAzIVzv8lhI","colab":{"base_uri":"https://localhost:8080/","height":289},"executionInfo":{"status":"error","timestamp":1684015076472,"user_tz":180,"elapsed":2533,"user":{"displayName":"Kevin Nicolas Costantino","userId":"03967773680110882059"}},"outputId":"733353ae-53a0-413e-9024-8d4205b6107d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-20-203095c4011e>:3: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n","  score = model.evaluate_generator(val_set,round(40/batch_size))\n"]},{"output_type":"stream","name":"stdout","text":[" Total:  23\n"]},{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-203095c4011e>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" Total: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilenames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loss: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Accuracy: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m#print(\"Accuracy = \",score[1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: list index out of range"]}]},{"cell_type":"code","source":["classifier.save('/content/drive/My Drive/Colab_Project/glaucoma/model.h5')\n"],"metadata":{"id":"usP4LFeo8oxH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","from keras.models import load_model\n","from PIL import Image\n","from keras.preprocessing import image\n","import numpy as np\n","import cv2"],"metadata":{"id":"3ZXvRbtd8qzI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["target_size = (256,256)\n","model=load_model('/content/drive/My Drive/Colab_Project/glaucoma/model.h5')\n","print(\"model loaded\")\n"],"metadata":{"id":"sSy51egP8tZg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1683930242984,"user_tz":180,"elapsed":1722,"user":{"displayName":"Kevin Nicolas Costantino","userId":"03967773680110882059"}},"outputId":"4ee48745-1445-453a-a7ad-8d363b1b7d8b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["model loaded\n"]}]},{"cell_type":"code","source":["import numpy as np\n","from keras.preprocessing import image\n","test_image = tf.keras.utils.load_img('/content/drive/My Drive/Colab_Project/glaucoma/train/class0/Im084.jpg', target_size = (256,256))\n","test_image = tf.keras.utils.img_to_array(test_image)\n","test_image = np.expand_dims(test_image, axis = 0)\n","result = model.predict(test_image)\n","training_set.class_indices\n","if result[0][0] == 1:\n"," print(\"Glaucoma\")\n","else:\n"," print(\"Not Glaucoma\")\n"],"metadata":{"id":"nJBTY_oq8vVw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from keras.preprocessing import image\n","test_image = tf.keras.utils.load_img('/content/drive/My Drive/compara_segmentadores/data2/A/ORIGA/test/classe 1 ts/036.jpg', target_size = (256,256))\n","test_image = tf.keras.utils.img_to_array(test_image)\n","\n","test_image = np.expand_dims(test_image, axis = 0)\n","#print(test_image)\n","result = model.predict(test_image)\n","print(result)\n","training_set.class_indices\n","if result[0][0] == 1:\n"," print(\"Glaucoma\")\n","else:\n"," print(\"Not Glaucoma\")"],"metadata":{"id":"8080Q07O8xew","colab":{"base_uri":"https://localhost:8080/","height":235},"executionInfo":{"status":"error","timestamp":1683944166893,"user_tz":180,"elapsed":7429,"user":{"displayName":"Kevin Nicolas Costantino","userId":"03967773680110882059"}},"outputId":"29c2fa3b-d705-45bf-b350-8f56cf532124"},"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-6cabec8e6484>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtest_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_img\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/compara_segmentadores/data2/A/ORIGA/test/classe 1 ts/036.jpg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtest_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimg_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_image\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"]}]},{"cell_type":"code","source":["import numpy as np \n","import pandas as pd \n","import matplotlib.pyplot as plt\n","from PIL import  Image\n","%matplotlib inline\n","plt.style.use('fivethirtyeight')\n","def plot_model_history(model_history):\n","    fig, axs = plt.subplots(1,2,figsize=(15,5))\n","    # summarize history for accuracy\n","    axs[0].plot(range(1,len(model_history.history['binary_accuracy'])+1),model_history.history['acc'],metrics=\"binary_accuracy\")\n","    axs[0].plot(range(1,len(model_history.history['val_acc'])+1),model_history.history['val_acc'])\n","    axs[0].set_title('Model Accuracy')\n","    axs[0].set_ylabel('Accuracy')\n","    axs[0].set_xlabel('Epoch')\n","    axs[0].set_xticks(np.arange(1,len(model_history.history['acc'])+1),len(model_history.history['acc'])/10)\n","    axs[0].legend(['train', 'val'], loc='best')\n","    # summarize history for loss\n","    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n","    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n","    axs[1].set_title('Model Loss')\n","    axs[1].set_ylabel('Loss')\n","    axs[1].set_xlabel('Epoch')\n","    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n","    axs[1].legend(['train', 'val'], loc='best')\n","    plt.show()\n","     "],"metadata":{"id":"xWUKO7D58zyf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_model_history(model_info)"],"metadata":{"id":"QaB6Td54816w","colab":{"base_uri":"https://localhost:8080/","height":626},"executionInfo":{"status":"error","timestamp":1683872570638,"user_tz":180,"elapsed":580,"user":{"displayName":"Kevin Nicolas Costantino","userId":"03967773680110882059"}},"outputId":"dcf83afc-b224-46a0-82b0-9ddac6dd09be"},"execution_count":null,"outputs":[{"output_type":"error","ename":"KeyError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-43-9e98b7b828a6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_model_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-42-3b1c0e83e917>\u001b[0m in \u001b[0;36mplot_model_history\u001b[0;34m(model_history)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# summarize history for accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'binary_accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"binary_accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'binary_accuracy'"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1500x500 with 2 Axes>"],"image/png":"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\n"},"metadata":{}}]}]}