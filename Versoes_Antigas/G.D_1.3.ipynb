{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":415},"executionInfo":{"elapsed":35659,"status":"error","timestamp":1661948814506,"user":{"displayName":"Kevin Nicolas Costantino","userId":"03967773680110882059"},"user_tz":240},"id":"eSL_QylvyLIo","outputId":"d273d245-4411-44f4-ea15-666be300128e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n","Vai ler as imagens de:  /content/drive/MyDrive/data/\n"]},{"ename":"FileNotFoundError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-1709ddce6f2c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     72\u001b[0m                                ])    \n\u001b[1;32m     73\u001b[0m \u001b[0;31m# Prepara banco de imagens de treino (está junto com validação por enquanto)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m \u001b[0mtraining_val_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpasta_treino\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;31m# Prepara banco de imagens de teste\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpasta_teste\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m             \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m             \u001b[0mis_valid_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_valid_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m         )\n\u001b[1;32m    318\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    143\u001b[0m     ) -> None:\n\u001b[1;32m    144\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    217\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \"\"\"\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \"\"\"\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/data/train'"]}],"source":["import torch   # Biblioteca pytorch principal\n","from torch import nn  # Módulo para redes neurais (neural networks)\n","from torch.utils.data import DataLoader # Manipulação de bancos de imagens\n","from torchvision import datasets,models # Ajuda a importar alguns bancos e\n","                                        # e modelos já prontos e famosos\n","import torchvision.transforms as transforms\n","import matplotlib.pyplot as plt # Mostra imagens e gráficos\n","from torch.utils.tensorboard import SummaryWriter # Salva \"log\" da aprendizagem\n","from torch.utils.data import Subset\n","import torchvision\n","import PIL  # Biblioteca para manipulação de imagens\n","import sklearn.metrics as metrics  # Ajuda a calcular métricas de desempenho\n","from sklearn.metrics import precision_recall_fscore_support as score\n","from sklearn.model_selection import train_test_split\n","import seaborn as sn  # Usado para gerar um mapa de calor para a matriz de confusão\n","import pandas as pd   # Ajuda a trabalhar com tabelas\n","import numpy as np    # Várias funções numéricas\n","\n","# Definindo alguns hiperparâmetros importantes:\n","epocas = 100  # Total de passagens durante a aprendizagem pelo conjunto de imagens\n","tamanho_lote = 16  # Tamanho de cada lote sobre o qual é calculado o gradiente\n","taxa_aprendizagem = 0.01   # Magnitude das alterações nos pesos\n","momento = 0.2  # Mantem informação de pesos anteriores (as mudanças de\n","               # de peso passam a ser mais suaves)\n","paciencia = 5  # Total de épocas sem melhoria da acurácia na validação até parar\n","tolerancia = 0.01 # Melhoria menor que este valor não é considerada melhoria\n","perc_val = 0.2    # Percentual do treinamento a ser usado para validação\n","\n","# Define uma arquitetura já conhecida que será usada\n","# Opções atuais: \"resnet\", \"squeezenet\", \"densenet\"\n","nome_rede = \"resnet\"\n","tamanho_imagens = 224  # Tamanho das imagens para estas arquiteturas\n","\n","\n","# Cria uma função para saber se estamos rodando de dentro de um notebook\n","# jupyter \n","def in_notebook():\n","    try:\n","        from IPython import get_ipython\n","        if 'IPKernelApp' not in get_ipython().config:  # pragma: no cover\n","            return False\n","    except ImportError:\n","        return False\n","    except AttributeError:\n","        return False\n","    return True\n","\n","\n","# Liga o Colab à sua conta no Drive\n","if in_notebook():\n","   from google.colab import drive\n","   drive.mount('/content/drive')\n","\n","# Ajusta nomes das pastas onde estão as imagens de treino e teste.\n","# As imagens de validação serão criadas através de um percentual das\n","# imagens de treino (como fazemos em compara_classificadores_tf2)\n","pasta_base = \"../../\"  # No desktop o programa estará na pasta \"src\"\n","if in_notebook(): pasta_base = \"/content/drive/MyDrive/\"\n","pasta_data = pasta_base+\"data/\"\n","print(\"Vai ler as imagens de: \",pasta_data)\n","pasta_treino = pasta_data+\"train\"\n","pasta_teste  = pasta_data+\"test\"\n","\n","# Define as transformações nas imagens: \n","# Muda tamanho, transforma em tensor e normaliza usando os valores\n","# calculados sobre a base ImageNet (por conta da transferência de aprendizagem)\n","#\n","# Nem sempre a normalização dá melhores resultados. Descomente a linha se quiser usar.\n","transform = transforms.Compose([transforms.Resize((tamanho_imagens,tamanho_imagens)),\n","                                transforms.ToTensor(),\n","                                #transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n","                               ])    \n","# Prepara banco de imagens de treino (está junto com validação por enquanto)\n","training_val_data = datasets.ImageFolder(root=pasta_treino,transform=transform)\n","# Prepara banco de imagens de teste\n","test_data = datasets.ImageFolder(root=pasta_teste,transform=transform)\n","\n","# Aqui vai separar em treinamento e validação\n","train_idx, val_idx = train_test_split(list(range(len(training_val_data))), test_size=perc_val)\n","training_data = Subset(training_val_data, train_idx)\n","val_data = Subset(training_val_data, val_idx)\n","\n","\n","# Cria os objetos que irão manipular os dados (basicamente ajuda a pegar\n","# lote (batch) de imagens de treinamento e de validação)\n","train_dataloader = DataLoader(training_data, batch_size=tamanho_lote,shuffle=True)\n","val_dataloader = DataLoader(val_data, batch_size=tamanho_lote,shuffle=True)\n","\n","# Mostra informações do primeiro lote de imagens de validação \n","# X vai conter um lote de imagens\n","# y vai conter as classes (tipo de vestimenta) de cada imagem do lote\n","for X, y in val_dataloader:\n","    print(f\"Tamanho do lote de imagens: {X.shape[0]}\")\n","    print(f\"Quantidade de canais: {X.shape[1]}\")\n","    print(f\"Altura de cada imagem: {X.shape[2]}\")\n","    print(f\"Largura de cada imagem: {X.shape[3]}\")\n","    print(f\"Tamanho do lote de classes (labels): {y.shape[0]}\")\n","    print(f\"Tipo de cada classe: {y.dtype}\")\n","    break  # Para depois de mostrar os dados do primeiro lote\n","\n","total_imagens=len(training_data)+len(val_data)+len(test_data)\n","print(f\"Total de imagens: {total_imagens}\")\n","print(f\"Total de imagens de treinamento: {len(training_data)} ({100*len(training_data)/total_imagens:>2f}%)\")\n","print(f\"Total de imagens de validação: {len(val_data)} ({100*len(val_data)/total_imagens:>2f}%)\")\n","print(f\"Total de imagens de teste: {len(test_data)} ({100*len(test_data)/total_imagens:>2f}%)\")\n","labels_map = {v: k for k, v in test_data.class_to_idx.items()}\n","print('\\nClasses:',labels_map)\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"lEZ-ZwlAyLIw"},"source":["### Mostrando algumas imagens"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":16,"status":"aborted","timestamp":1661948814510,"user":{"displayName":"Kevin Nicolas Costantino","userId":"03967773680110882059"},"user_tz":240},"id":"MsKtM0EU9P0s"},"outputs":[],"source":["figure = plt.figure(figsize=(8, 8))  # Cria o local para mostrar as imagens\n","cols, rows = 3, 3  # Irá mostrar 9 imagens em uma grade 3x3\n","for i in range(1, cols * rows + 1):\n","    # Gera um número aleatório menor que o total de imagens disponíveis\n","    sample_idx = torch.randint(len(training_data), size=(1,)).item()\n","    # Pega a imagem e sua classificação usando o número aleatório\n","    img, label = training_data[sample_idx]\n","    # Adiciona a imagem na grade que será mostrada\n","    figure.add_subplot(rows, cols, i)\n","    # Usa a classe da imagem como título da imagem\n","    plt.title(labels_map[label])\n","    # Não mostra valores para os eixos X e Y\n","    plt.axis(\"off\")\n","    # Tem que ajustar a ordem das dimensões do tensor para que os canais\n","    # fiquem na última dimensão (e não ma primeira)\n","    plt.imshow(img.permute(1,2,0))\n","    \n","plt.show() # Este é o comando que vai mostrar as imagens\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"clbTkvdi_-0T"},"source":["## Definindo uma rede neural artificial"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":16,"status":"aborted","timestamp":1661948814511,"user":{"displayName":"Kevin Nicolas Costantino","userId":"03967773680110882059"},"user_tz":240},"id":"QtXUytcuyLIx"},"outputs":[],"source":["# Verifica se tem GPU na máquina, caso contrário, usa a CPU mesmo\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(f\"Usando {device}\")\n","\n","# Vai precisar do total de classes para ajustar a última camada\n","# das redes\n","\n","total_classes = len(labels_map)\n","# Inicia com a rede escolhida (acrescente mais \"ifs\" para outras redes)\n","# Carrega os pesos pré-treinados na ImageNet (transfer learning)\n","# Ajusta a última camada para poder corresponder ao total de classes do\n","# problema atual. \n","if nome_rede == \"resnet\":\n","   model = models.resnet18(pretrained=True)\n","   model.fc = nn.Linear(model.fc.in_features, total_classes )\n","elif nome_rede == \"squeezenet\":\n","   model = models.squeezenet1_0(pretrained=True)\n","   model.classifier[1] = nn.Conv2d(512, total_classes, kernel_size=(1,1), stride=(1,1))\n","   model.num_classes = total_classes\n","elif nome_rede == \"densenet\":\n","   model = models.densenet161(pretrained=True)\n","   model.classifier = nn.Linear(model.classifier.in_features,total_classes)\n","        \n","\n","# Prepara a rede para o dispositivo que irá processá-la\n","model = model.to(device)\n","\n","# Imprime dados sobre a arquitetura da rede\n","print(model)\n","\n","# Define o otimizador como sendo descida de gradiente estocástica\n","otimizador = torch.optim.SGD(model.parameters(), lr=taxa_aprendizagem, momentum=momento)\n","\n","# Define a função de perda como entropia cruzada\n","funcao_perda = nn.CrossEntropyLoss()\n","\n","# Cria o módulo do tensorboard de coleta de dados\n","writer = SummaryWriter()\n","\n","# Define a função para treinar a rede\n","# dataloader = módulo que manipula o conjunto de imagens\n","# model = arquitetura da rede\n","# loss_fn = função de perda\n","# optimizer = otimizador \n","def train(dataloader, model, loss_fn, optimizer):\n","    size = len(dataloader.dataset)  # Total de imagens\n","    num_batches = len(dataloader)   # Total de lotes\n","    model.train()  # Avisa que a rede vai entrar em modo de aprendizagem\n","\n","    train_loss, train_correct = 0, 0  # Usado para calcular perda e acurácia médias\n","\n","    # Pega um lote de imagens de cada vez do conjunto de treinamento\n","    for batch, (X, y) in enumerate(dataloader):\n","\n","        X, y = X.to(device), y.to(device)  # Prepara os dados para o dispositivo (GPU ou CPU)\n","        pred = model(X)         # Realiza uma previsão usando os pesos atuais\n","        loss = loss_fn(pred, y) # Calcula o erro com os pesos atuais\n","\n","        train_loss += loss.item() # Guarda para calcular a perda média\n","        # Calcula os acertos para o lote inteiro de imagens\n","        train_correct += (pred.argmax(1) == y).type(torch.float).sum().item() \n","\n","\n","        loss.backward()        # Calcula os gradientes com base no erro (loss)\n","        optimizer.step()       # Ajusta os pesos com base nos gradientes\n","        optimizer.zero_grad()  # Zera os gradientes pois vai acumular para todas\n","                               # as imagens do lote\n","\n","        # Imprime informação a cada 4 lotes processados \n","        if batch % 4 == 0:\n","            # Mostra a perda e o total de imagens já processadas\n","            loss, current = loss.item(), batch * len(X)\n","            print(f\"Perda Treino: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n","\n","    train_loss /= num_batches  # Como a perda foi calculada por lote, divide\n","                               # pelo total de lotes para calcular a média\n","    train_acuracia = train_correct / size  # Já o total de acertos é em relação\n","                                           # ao total geral de imagens\n","\n","    return train_loss, train_acuracia        \n","\n","# Define a função de validação (aqui a rede não está aprendendo, apenas\n","# usando \"aquilo que aprendeu\", mas em um conjunto de imagens diferente\n","# do conjunto usado para aprender)\n","def validation(dataloader, model, loss_fn):\n","    size = len(dataloader.dataset)  # Total de imagens para validação\n","    num_batches = len(dataloader)   # Total de lotes\n","    model.eval()  # Coloca a rede em modo de avaliação (e não de aprendizagem)\n","    \n","    # Vai calcular a perda e o total de acertos no conjunto de validação\n","    val_loss, val_correct = 0, 0\n","\n","    # Na validação os pesos não são ajustados e por isso não precisa\n","    # calcular o gradiente\n","    with torch.no_grad():\n","        for X, y in dataloader:\n","            X, y = X.to(device), y.to(device)\n","            pred = model(X)\n","            val_loss += loss_fn(pred, y).item()\n","            val_correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n","\n","    val_loss /= num_batches\n","    val_acuracia = val_correct / size\n","\n","    print(\"Informações na Validação:\")\n","    print(f\"Total de acertos: {int(val_correct)}\")\n","    print(f\"Total de imagens: {size}\")\n","    print(f\"Perda média: {val_loss:>8f}\")            \n","    print(f\"Acurácia: {(100*val_acuracia)}%\")\n","    return val_loss, val_acuracia\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"jE_dKeYwyLIy"},"source":["## Treinando a Rede Neural (Aprendizagem)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":17,"status":"aborted","timestamp":1661948814512,"user":{"displayName":"Kevin Nicolas Costantino","userId":"03967773680110882059"},"user_tz":240},"id":"RcLeXvAzyLI1"},"outputs":[],"source":["# A aprendizagem agora tem parada antecipada (early stopping)\n","\n","maior_acuracia = 0  # Guarda a melhor acurácia no conjunto de validação\n","total_sem_melhora = 0  # Guarda quantas épocas passou sem melhoria na acurácia\n","\n","# Passa por todas as imagens várias vezes (a quantidade de vezes\n","# é definida pelo hiperparâmetro \"epocas\")\n","for epoca in range(epocas):\n","    print(f\"-------------------------------\")\n","    print(f\"Época {epoca+1} \\n-------------------------------\")\n","    train_loss, train_acuracia = train(train_dataloader, model, funcao_perda, otimizador)\n","    val_loss, val_acuracia = validation(val_dataloader, model, funcao_perda)\n","\n","    # Guarda informações para o tensorboard pode criar os gráficos depois\n","    writer.add_scalars('Loss', {'train':train_loss,'val':val_loss}, epoca)\n","    writer.add_scalars('Accuracy', {'train':train_acuracia,'val':val_acuracia}, epoca)\n","\n","    # Soma uma tolerancia no valor da maior acurácia para que melhoras muito\n","    # pequenas não sejam consideradas\n","    if val_acuracia > (maior_acuracia+tolerancia): \n","      # Salva a melhor rede encontrada até o momento\n","      torch.save(model.state_dict(), \"modelo_treinado_\"+nome_rede+\".pth\")\n","      print(\"Salvou o modelo com a maior acurácia na validação até agora em modelo_treinado_\"+nome_rede+\".pth\")      \n","      maior_acuracia = val_acuracia\n","      total_sem_melhora = 0\n","    else: \n","      total_sem_melhora += 1 \n","      print(f\"Sem melhora há {total_sem_melhora} épocas ({100*val_acuracia}% <= {100*(maior_acuracia+tolerancia)}%)\")\n","    if total_sem_melhora > paciencia:\n","      print(f\"Acabou a paciência com {epoca+1} épocas \")\n","      break\n","\n","print(\"Terminou a fase de aprendizagem !\")\n","\n","# Pega algumas imagens para o tensorboard mostrar depois\n","images, labels = iter(train_dataloader).next()\n","images = images.to(device)\n","labels = labels.to(device)\n","# Cria uma grade de imagens para o tensorboard\n","img_grid = torchvision.utils.make_grid(images)\n","writer.add_image('Minhas Imagens', img_grid)\n","writer.add_graph(model, images)\n","writer.close()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"jj3oPeru6zvU"},"source":["## Visualização usando Tensorboard\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":17,"status":"aborted","timestamp":1661948814513,"user":{"displayName":"Kevin Nicolas Costantino","userId":"03967773680110882059"},"user_tz":240},"id":"_mWahNMU6yk2"},"outputs":[],"source":["%load_ext tensorboard\n","%tensorboard --logdir=runs"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"vRjA7xFPyLI3"},"source":["## Carregando a rede treinada anteriormente e usando\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":19,"status":"aborted","timestamp":1661948814515,"user":{"displayName":"Kevin Nicolas Costantino","userId":"03967773680110882059"},"user_tz":240},"id":"PDVI2nbZyLI3"},"outputs":[],"source":["model.load_state_dict(torch.load(\"modelo_treinado_\"+nome_rede+\".pth\"))\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"8VNfxKGJyLI3"},"source":["## Usando a rede treinada para classificar algumas imagens "]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":36677,"status":"aborted","timestamp":1661948814516,"user":{"displayName":"Kevin Nicolas Costantino","userId":"03967773680110882059"},"user_tz":240},"id":"ajhaKszXyLI4"},"outputs":[],"source":["# Classifica uma única imagem \n","# model: rede a ser usada\n","# x: imagem\n","# y: classificação real da imagem\n","# predita: classificação dada pela rede\n","def classifica_uma_imagem(model,x,y):\n","    model.eval()\n","    with torch.no_grad():\n","       pred = model(x)\n","       predita, real = labels_map[int(pred[0].argmax(0))], labels_map[y]\n","       print(f'Predita: \"{predita}\", Real: \"{real}\"')\n","    return(predita)\n","\n","# Vai mostrar a classificação da rede para 16 imagens do conjunto de teste\n","figure = plt.figure(figsize=(8, 8))  # Cria o local para mostrar as imagens\n","cols, rows = 4, 4  # Irá mostrar 16 imagens em uma grade 4x4\n","print(f\"Testando em {len(test_data)} imagens. Resultados:\")\n","for i in range(cols*rows):\n","    aleatoria = torch.randint(len(test_data), size=(1,)).item()\n","    img, label = test_data[aleatoria]\n","\n","    img = img.unsqueeze(0).to(device)\n","\n","    # Classifica a imagem usando a rede treinada\n","    predita = classifica_uma_imagem(model,img,label)\n","    # Adiciona a imagem na grade que será mostrada\n","    figure.add_subplot(rows, cols, i+1)\n","    # Usa a classe da imagem como título da imagem\n","    plt.title(predita)\n","    # Não mostra valores para os eixos X e Y\n","    plt.axis(\"off\")\n","    # Primeiro converte o tensor para o formato de cpu (o imshow não vai usar GPU)\n","    # Depois retira a dimensão relacionada com os lotes (dimensão 0)\n","    # e por fim, faz uma permutação das dimensões de forma que\n","    # a dimensão relacionada aos canais (RGB) fiquem por último e não em primeiro\n","    # fiquem na última dimensão (e não ma primeira)\n","    plt.imshow(img.cpu().squeeze(0).permute(1,2,0))\n","    \n","plt.show() # Este é o comando que vai mostrar as imagens\n","\n","\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"bg3aK5kL_Jya"},"source":["## Gera matriz de confusão e algumas métricas de avaliação"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":36673,"status":"aborted","timestamp":1661948814518,"user":{"displayName":"Kevin Nicolas Costantino","userId":"03967773680110882059"},"user_tz":240},"id":"Ny6ulLuJ_OcQ"},"outputs":[],"source":["# Listas para guardar valores preditos e reais\n","predicoes = []\n","reais = []\n","\n","# Vai acumular acertos para calcular acurácia\n","test_correct=0\n","\n","model.eval() # Coloca a rede no modo de avaliação (e não de aprendizagem)\n","with torch.no_grad():   # Avisa que não devem ser calculados gradientes\n","   for img, label in test_data:   # Para cada imagem do conjunto de teste\n","      img = img.unsqueeze(0).to(device)\n","      predicao = model(img)       # Faz a predição usando a rede\n","      predicao = int(predicao[0].argmax(0))  # Pega a classe com maior valor\n","      predicoes.extend([predicao]) # Guarda predição na lista\n","      reais.extend([label])        # Guarda valor real na lista\n","      test_correct += (predicao == label)\n","\n","# Acurácia no conjunto de teste\n","test_acuracia = test_correct/len(test_data)\n","\n","# Constroi a matriz de confusão\n","matriz = metrics.confusion_matrix(reais,predicoes)\n","\n","# Pega a lista de classes \n","classes=list(labels_map.values())\n","\n","# Normaliza a matriz para o intervalo 0 e 1 e arredonda em 2 casas decimais \n","# cada célula\n","matriz_normalizada = np.round(matriz/np.sum(matriz) * len(labels_map),2)\n","# Transforma a matriz no formato da biblioteca PANDA\n","df_matriz = pd.DataFrame(matriz_normalizada, index = classes,\n","                     columns = [i for i in classes])\n","\n","# Gera uma imagem do tipo mapa de calor\n","plt.figure(figsize = (12,7))\n","sn.heatmap(df_matriz, annot=True)\n","plt.savefig('matriz_confusao.png')\n","\n","print('Métricas de desempenho no conjunto de teste:')\n","print(metrics.classification_report(reais,predicoes))\n","\n","precision,recall,fscore,support=score(reais,predicoes,average='macro')\n","print('-----------------------------------')\n","print(f'Resumo para as {len(test_data)} imagens de teste:')\n","print(f\"Acertos: {int(test_correct)}\")\n","print(f\"Acurácia: {(100*test_acuracia):>0.2f}%\")\n","print(f\"Precisão: {100*precision:>0.2f}%\")\n","print(f\"Revocação: {100*recall:>0.2f}%\")\n","print(f\"Medida-F: {100*fscore:>0.2f}%\")\n","print('-----------------------------------')\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Cópia de exemplo_pytorch_v4.ipynb","provenance":[{"file_id":"1egrQOlXvi_rvX2ZtfvK56wVXMyIp6GCh","timestamp":1661948711341},{"file_id":"1-GMeHTbbz4MqqUDOMPkd2cLT0rMIBU8k","timestamp":1660069468550},{"file_id":"1eqZbgFoN2GLNFBreSx1Rp7DmU6b3Di7E","timestamp":1659722042962},{"file_id":"1sJJgfc_2wLvvZWwhz2Ea8oWUxS9IcORu","timestamp":1659462560726},{"file_id":"https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/af0caf6d7af0dda755f4c9d7af9ccc2c/quickstart_tutorial.ipynb","timestamp":1659381789765}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.13"}},"nbformat":4,"nbformat_minor":0}
