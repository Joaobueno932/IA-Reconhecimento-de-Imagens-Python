{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNlqSpDJwF3WJdpxKk3i2x1"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28961,"status":"ok","timestamp":1694137241964,"user":{"displayName":"Kevin Nicolas Costantino","userId":"03967773680110882059"},"user_tz":180},"id":"lUrlhqFGvfB3","outputId":"035bfe13-c891-41a9-d93c-7d8da51f58ae"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","import torchvision.datasets as datasets\n","from torch.utils.data import DataLoader\n","from torch.utils.data.dataset import Dataset\n","import os\n","from google.colab import drive\n","\n","# Montar o Google Drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1556,"status":"ok","timestamp":1694137244490,"user":{"displayName":"Kevin Nicolas Costantino","userId":"03967773680110882059"},"user_tz":180},"id":"XwXtnga0QIjf","outputId":"54739d36-454c-417e-b5e7-cb8dc9a1188a"},"outputs":[{"output_type":"stream","name":"stdout","text":["['class 0', 'class 1']\n"]}],"source":["print(os.listdir(('/content/drive/MyDrive/compara_segmentadores/data2/talho/G1020/train/images/')))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_Lbcip8HxniI"},"outputs":[],"source":["!pip install torch torchvision opencv-python\n","!pip install scikit-plot\n","!pip install django-model-utils"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":390},"executionInfo":{"elapsed":1500,"status":"error","timestamp":1694137252745,"user":{"displayName":"Kevin Nicolas Costantino","userId":"03967773680110882059"},"user_tz":180},"id":"NQ4jLiBSvUQr","outputId":"b73b336a-d80d-4eae-ddfd-bd8b0d464814"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-b79d734f02ab>\u001b[0m in \u001b[0;36m<cell line: 80>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m# Carregar os dados\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;31m# Inicializar o encoder e o decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-b79d734f02ab>\u001b[0m in \u001b[0;36mload_data\u001b[0;34m(root_dir)\u001b[0m\n\u001b[1;32m     51\u001b[0m     )\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     test_data = torchvision.datasets.ImageFolder(\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mroot\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimage_dir_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader, is_valid_file)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mis_valid_file\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m     ):\n\u001b[0;32m--> 309\u001b[0;31m         super().__init__(\n\u001b[0m\u001b[1;32m    310\u001b[0m             \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform, is_valid_file)\u001b[0m\n\u001b[1;32m    142\u001b[0m     ) -> None:\n\u001b[1;32m    143\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_transform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_valid_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(self, directory)\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mall\u001b[0m \u001b[0mclasses\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mdictionary\u001b[0m \u001b[0mmapping\u001b[0m \u001b[0meach\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mto\u001b[0m \u001b[0man\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m         \"\"\"\n\u001b[0;32m--> 218\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(directory)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0mSee\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDatasetFolder\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdetails\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \"\"\"\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscandir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Couldn't find any class folder in {directory}.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/compara_segmentadores/data2/talho/G1020/valid/images'"]}],"source":["import os\n","import torch\n","import torchvision\n","from torchvision import transforms\n","import torch.utils.data as data\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score, precision_recall_curve, accuracy_score, precision_score, recall_score, f1_score\n","import seaborn as sn\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","# Definir a arquitetura do encoder\n","class Encoder(nn.Module):\n","    def __init__(self):\n","        super(Encoder, self).__init__()\n","        self.encoder = torchvision.models.resnet18(pretrained=True)\n","        self.encoder.fc = nn.Identity()  # Remover a última camada totalmente conectada\n","\n","    def forward(self, x):\n","        x = self.encoder(x)\n","        return x\n","\n","# Definir a arquitetura do decoder\n","class Decoder(nn.Module):\n","    def __init__(self):\n","        super(Decoder, self).__init__()\n","        self.decoder = nn.Linear(512, num_classes)  # Substituir 512 pelo número de classes do seu problema\n","\n","    def forward(self, x):\n","        x = self.decoder(x)\n","        return x\n","\n","# Função para carregar as imagens e anotações\n","def load_data(root_dir):\n","    image_dir_train = os.path.join(root_dir, 'train/images')\n","    annotation_dir_train = os.path.join(root_dir, 'train/anotacoes')\n","    image_dir_test = os.path.join(root_dir, 'valid/images')\n","    annotation_dir_test = os.path.join(root_dir, 'valid/anotacoes')\n","\n","    transform = transforms.Compose([\n","        transforms.Resize((224, 224)),  # Redimensionar todas as imagens para o tamanho 224x224\n","        transforms.ToTensor()\n","    ])\n","\n","    train_data = torchvision.datasets.ImageFolder(\n","        root=image_dir_train,\n","        transform=transform\n","    )\n","\n","    test_data = torchvision.datasets.ImageFolder(\n","        root=image_dir_test,\n","        transform=transform\n","    )\n","\n","    train_loader = torch.utils.data.DataLoader(\n","        train_data,\n","        batch_size=16,\n","        shuffle=True\n","    )\n","\n","    test_loader = torch.utils.data.DataLoader(\n","        test_data,\n","        batch_size=16,\n","        shuffle=False\n","    )\n","\n","    return train_loader, test_loader\n","\n","# Definir os parâmetros\n","root_dir = '/content/drive/MyDrive/compara_segmentadores/data2/talho/G1020/'\n","num_classes = 2  # Substituir pelo número de classes do seu problema\n","encoder_lr = 0.01\n","decoder_lr = 0.01\n","num_epochs = 3\n","\n","# Carregar os dados\n","train_loader, test_loader = load_data(root_dir)\n","\n","# Inicializar o encoder e o decoder\n","encoder = Encoder()\n","decoder = Decoder()\n","\n","# Definir a função de perda e os otimizadores\n","criterion = nn.CrossEntropyLoss()\n","encoder_optimizer = optim.Adam(encoder.parameters(), lr=encoder_lr)\n","decoder_optimizer = optim.Adam(decoder.parameters(), lr=decoder_lr)\n","\n","# Treinamento\n","for epoch in range(num_epochs):\n","    encoder.train()\n","    decoder.train()\n","\n","    for images, labels in train_loader:\n","        encoder_optimizer.zero_grad()\n","        decoder_optimizer.zero_grad()\n","\n","        # Passar as imagens pelo encoder\n","        features = encoder(images)\n","\n","        # Passar as features pelo decoder\n","        outputs = decoder(features)\n","\n","        # Calcular a perda\n","        loss = criterion(outputs, labels)\n","\n","        # Retropropagação e otimização\n","        loss.backward()\n","        encoder_optimizer.step()\n","        decoder_optimizer.step()\n","\n","    # Avaliação\n","encoder.eval()\n","decoder.eval()\n","\n","true_labels = []\n","predicted_labels = []\n","\n","with torch.no_grad():\n","    for images, labels in test_loader:\n","        # Passar as imagens pelo encoder\n","        features = encoder(images)\n","\n","        # Passar as features pelo decoder\n","        outputs = decoder(features)\n","\n","        # Calcular as predições\n","        _, predicted = torch.max(outputs, 1)\n","        true_labels.extend(labels.cpu().numpy())\n","        predicted_labels.extend(predicted.cpu().numpy())\n","\n","# Calcular métricas\n","accuracy = accuracy_score(true_labels, predicted_labels)\n","conf_matrix = confusion_matrix(true_labels, predicted_labels)\n","class_report = classification_report(true_labels, predicted_labels, target_names=[f'Class {i}' for i in range(num_classes)])\n","\n","# Imprimir métricas\n","print(f'Accuracy: {accuracy}')\n","print('Confusion Matrix:')\n","print(conf_matrix)\n","print('Classification Report:')\n","print(class_report)\n","\n","# Visualizar matriz de confusão\n","df_cm = pd.DataFrame(conf_matrix, index=[i for i in range(num_classes)], columns=[i for i in range(num_classes)])\n","plt.figure(figsize=(10, 7))\n","sn.heatmap(df_cm, annot=True, fmt=\"d\")\n","plt.xlabel('Predicted')\n","plt.ylabel('True')\n","plt.show()\n","\n","# Salvar os modelos treinados\n","torch.save(encoder.state_dict(), 'encoder.pth')\n","torch.save(decoder.state_dict(), 'decoder.pth')\n"]},{"cell_type":"code","source":["import os\n","import torch\n","import torchvision\n","from torchvision import transforms\n","import torch.utils.data as data\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score, precision_recall_curve, accuracy_score, precision_score, recall_score, f1_score\n","import seaborn as sn\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","import os\n","import torch\n","import torchvision\n","from torchvision import transforms\n","import torch.utils.data as data\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from sklearn.metrics import confusion_matrix, classification_report, roc_curve, roc_auc_score, precision_recall_curve, accuracy_score, precision_score, recall_score, f1_score\n","import seaborn as sn\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import numpy as np\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n","\n","# Define a arquitetura da U-Net\n","class UNet(nn.Module):\n","    def __init__(self):\n","        super(UNet, self).__init__()\n","        # Encoder\n","        self.encoder = torchvision.models.resnet18(pretrained=True)\n","        self.encoder.fc = nn.Identity()  # Remove a camada fully connected\n","\n","        # Decoder\n","        self.decoder = nn.Sequential(\n","            nn.ConvTranspose2d(512, 256, kernel_size=3, stride=2, padding=1, output_padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=2, padding=1, output_padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=2, padding=1, output_padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.ConvTranspose2d(64, 1, kernel_size=1)  # Saída binária para segmentação\n","        )\n","\n","    def forward(self, x):\n","        # Encoder\n","        x = self.encoder(x)\n","\n","        # Decoder\n","        x = self.decoder(x)\n","        return x\n","\n","# Função para carregar as imagens e anotações\n","def load_data(root_dir):\n","    image_dir_train = os.path.join(root_dir, 'train/images')\n","    annotation_dir_train = os.path.join(root_dir, 'train/anotacoes')\n","\n","\n","    transform = transforms.Compose([\n","        transforms.Resize((224, 224)),  # Redimensionar todas as imagens para o tamanho 224x224\n","        transforms.ToTensor()\n","    ])\n","\n","    train_data = torchvision.datasets.ImageFolder(\n","        root=image_dir_train,\n","        transform=transform\n","    )\n","    train_loader = torch.utils.data.DataLoader(\n","        train_data,\n","        batch_size=16,\n","        shuffle=True\n","    )\n","    return train_loader\n","\n","def test_load_data(root_dir):\n","    image_dir_test = os.path.join(root_dir, 'test/images')\n","    annotation_dir_test = os.path.join(root_dir, 'test/anotacoes')\n","\n","    transform = transforms.Compose([\n","        transforms.Resize((224, 224)),  # Redimensionar todas as imagens para o tamanho 224x224\n","        transforms.ToTensor()\n","    ])\n","    test_data = torchvision.datasets.ImageFolder(\n","        root=image_dir_test,\n","        transform=transform\n","    )\n","    test_loader = torch.utils.data.DataLoader(\n","        test_data,\n","        batch_size=16,\n","        shuffle=False\n","    )\n","\n","    return test_loader\n","\n","def evaluate_model(model, data_loader):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model = model.to(device)\n","    model.eval()\n","    correct = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for inputs, labels in data_loader:\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","\n","            outputs = model(inputs)\n","            _, predicted = torch.max(outputs.data, 1)\n","\n","            total += labels.size(0)\n","            correct += (predicted == labels).sum().item()\n","\n","    accuracy = correct / total\n","    return accuracy\n","\n","def validation_load_data(root_dir):  # Corrigir o nome do parâmetro para root_dir\n","    image_dir_val = os.path.join(root_dir, 'valid/images')\n","    annotation_dir_val = os.path.join(root_dir, 'valid/anotacoes')\n","\n","    transform = transforms.Compose([\n","        transforms.Resize((224, 224)),\n","        transforms.ToTensor()\n","    ])\n","\n","    val_data = torchvision.datasets.ImageFolder(\n","        root=image_dir_val,\n","        transform=transform\n","    )\n","    val_loader = torch.utils.data.DataLoader(\n","        val_data,\n","        batch_size=16,\n","        shuffle=False\n","    )\n","\n","    return val_loader\n","\n","\n","# Dentro da função train_model, substitua a criação do modelo ResNet pela criação do modelo Encoder-Decoder\n","def train_model(train_loader, val_loader, num_epochs, MODELO):\n","    model = UNet()  # Use a U-Net em vez de ResNet\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","    model = model.to(device)\n","\n","    criterion = nn.BCEWithLogitsLoss()  # Use uma função de perda apropriada para segmentação binária\n","    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","\n","\n","    best_accuracy = 0.0  # Acompanhe a melhor precisão de validação\n","    train_accuracies = []  # Lista para armazenar as precisões de treinamento\n","    train_losses = []  # Lista para armazenar as perdas de treinamento\n","\n","    best_val_accuracy = 0.0  # Acompanhe a melhor precisão de validação\n","    val_accuracies = []  # Lista para armazenar as precisões de validação\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        running_loss = 0.0\n","\n","        for inputs, labels in train_loader:\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","\n","            optimizer.zero_grad()\n","\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","\n","        epoch_loss = running_loss / len(train_loader)\n","        train_losses.append(epoch_loss)\n","\n","        train_accuracy = evaluate_model(model, train_loader)\n","        train_accuracies.append(train_accuracy)\n","\n","        # Validação após cada época\n","        model.eval()\n","        val_accuracy = evaluate_model(model, val_loader)\n","        val_accuracies.append(val_accuracy)\n","\n","        print(f'Época [{epoch+1}/{num_epochs}], Precisão de Treinamento: {train_accuracy:.2%}, Precisão de Validação: {val_accuracy:.2%}, Perda: {epoch_loss:.4f}')\n","\n","        # Salvar o melhor modelo com base na maior precisão de validação\n","        if val_accuracy > best_accuracy:\n","            best_accuracy = val_accuracy\n","            best_model_state = model.state_dict()\n","\n","    # Salvar o checkpoint do melhor modelo\n","    torch.save(best_model_state, MODELO)\n","\n","    return model\n","\n","\n","def test_model(model, test_loader):\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Define o dispositivo de execução\n","    model.eval()\n","    y_true = []\n","    y_pred = []\n","    y_scores = []\n","\n","    with torch.no_grad():\n","        for inputs, labels in test_loader:\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","\n","            outputs = model(inputs)\n","            _, predicted = torch.max(outputs.data, 1)\n","\n","            y_true.extend(labels.cpu().numpy())\n","            y_pred.extend(predicted.cpu().numpy())\n","\n","            # Obter as probabilidades de saída (scores)\n","            scores = F.softmax(outputs, dim=1)\n","            y_scores.extend(scores[:, 1].cpu().numpy())\n","\n","    # Calcula a matriz de confusão\n","    cm = confusion_matrix(y_true, y_pred)\n","    tn, fp, fn, tp = cm.ravel()\n","    accuracy = accuracy_score(y_true, y_pred)\n","    precision = precision_score(y_true, y_pred)\n","    recall = recall_score(y_true, y_pred)\n","    f1 = f1_score(y_true, y_pred)\n","\n","    print(\"Acurácia:\", accuracy)\n","    print(\"Precisão:\", precision)\n","    print(\"Recall:\", recall)\n","    print(\"F1-score:\", f1)\n","\n","    print(\"True Negative (TN):\", tn)\n","    print(\"False Positive (FP):\", fp)\n","    print(\"False Negative (FN):\", fn)\n","    print(\"True Positive (TP):\", tp)\n","    class_names = ['Classe 0 (Sem Glaucoma)', 'Classe 1 (Com Glaucoma)']\n","\n","    # Plot da matriz de confusão\n","    plt.figure(figsize=(8, 6))\n","    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n","    plt.title('Matriz de Confusão')\n","    plt.colorbar()\n","    tick_marks = np.arange(len(class_names))\n","    plt.xticks(tick_marks, class_names, rotation=45)\n","    plt.yticks(tick_marks, class_names)\n","    plt.xlabel('Predito')\n","    plt.ylabel('Verdadeiro')\n","    plt.show()\n","\n","    # Relatório de classificação\n","    print(\"Relatório de Classificação:\")\n","    print(classification_report(y_true, y_pred, target_names=class_names))\n","\n","    # Calcular as estatísticas ROC\n","    fpr, tpr, thresholds_roc = roc_curve(y_true, y_scores)\n","    roc_auc = roc_auc_score(y_true, y_scores)\n","\n","    # Plot do gráfico ROC\n","    plt.figure(figsize=(8, 6))\n","    plt.plot(fpr, tpr, label='Curva ROC (AUC = {:.2f})'.format(roc_auc))\n","    plt.plot([0, 1], [0, 1], 'k--')\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('Taxa de Falso Positivo')\n","    plt.ylabel('Taxa de Verdadeiro Positivo')\n","    plt.title('Gráfico ROC')\n","    plt.legend(loc='lower right')\n","    plt.show()\n","\n","    # Calcular as estatísticas RC\n","    precision, recall, thresholds_rc = precision_recall_curve(y_true, y_scores)\n","\n","    # Plot do gráfico RC\n","    plt.figure(figsize=(8, 6))\n","    plt.plot(recall, precision, label='Curva RC')\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('Recall')\n","    plt.ylabel('Precision')\n","    plt.title('Gráfico RC')\n","    plt.legend(loc='lower left')\n","    plt.show()\n","\n","    # Relatório de classificação\n","    print(\"Relatório de Classificação:\")\n","    print(classification_report(y_true, y_pred, target_names=class_names))\n","\n","\n","Valid_root = \"/content/drive/MyDrive/compara_segmentadores/data2/talho/ORIGA/\"\n","Teste_root = \"/content/drive/MyDrive/compara_segmentadores/data2/talho/G1020/\"\n","num_epochs = 3\n","\n","Val_loader = validation_load_data(Valid_root)\n","Test_loader = test_load_data(Teste_root)\n","\n","# Carregar os dados para G1020\n","TrainG1020_root = \"/content/drive/MyDrive/compara_segmentadores/data2/talho/G1020/\"\n","Train_loader_G1020 = load_data(TrainG1020_root)\n","G1020M = 'G1020best_model.pth'\n","modelo_G1020 = train_model(Train_loader_G1020, Val_loader, num_epochs,G1020M)\n","melhor_estado_modelo_G1020 = torch.load(G1020M)\n","modelo_G1020.load_state_dict(melhor_estado_modelo_G1020)\n","test_model(modelo_G1020, Test_loader)\n","\n","# Carregar os dados para ORIGA\n","TrainORIGA_root = \"/content/drive/MyDrive/compara_segmentadores/data2/talho/ORIGA/\"\n","train_loader_ORIGA = load_data(TrainORIGA_root)\n","ORIGM = 'ORIGAbest_model.pth'\n","modelo_ORIGA = train_model(train_loader_ORIGA, Val_loader, num_epochs,ORIGM)\n","melhor_estado_modelo_ORIGA = torch.load(ORIGM)\n","modelo_ORIGA.load_state_dict(melhor_estado_modelo_ORIGA)\n","test_model(modelo_ORIGA, Test_loader)\n","\n","# Carregar os dados para REFUGE\n","TrainREFUGE_root = \"/content/drive/MyDrive/compara_segmentadores/data2/talho/REFUGE/\"\n","train_loader_REFUGE = load_data(TrainREFUGE_root)\n","REFUM = 'REFUGEbest_model.pth'\n","modelo_REFUGE = train_model(train_loader_REFUGE, Val_loader, num_epochs,REFUM)\n","melhor_estado_modelo_REFUGE = torch.load('REFUGEbest_model.pth')\n","modelo_REFUGE.load_state_dict(melhor_estado_modelo_REFUGE)\n","test_model(modelo_REFUGE, Test_loader)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":390},"id":"ngsjDdY8bdl2","executionInfo":{"status":"error","timestamp":1694142792584,"user_tz":180,"elapsed":2703,"user":{"displayName":"Kevin Nicolas Costantino","userId":"03967773680110882059"}},"outputId":"0aad242f-9b59-43d8-d336-20ace680a637"},"execution_count":7,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-95253dfd3488>\u001b[0m in \u001b[0;36m<cell line: 304>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[0mTrain_loader_G1020\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrainG1020_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0mG1020M\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'G1020best_model.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m \u001b[0mmodelo_G1020\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTrain_loader_G1020\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVal_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mG1020M\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0mmelhor_estado_modelo_G1020\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG1020M\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0mmodelo_G1020\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmelhor_estado_modelo_G1020\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-95253dfd3488>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_loader, val_loader, num_epochs, MODELO)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-7-95253dfd3488>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# Decoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, output_size)\u001b[0m\n\u001b[1;32m    954\u001b[0m             num_spatial_dims, self.dilation)  # type: ignore[arg-type]\n\u001b[1;32m    955\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 956\u001b[0;31m         return F.conv_transpose2d(\n\u001b[0m\u001b[1;32m    957\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m             output_padding, self.groups, self.dilation)\n","\u001b[0;31mRuntimeError\u001b[0m: Expected 3D (unbatched) or 4D (batched) input to conv_transpose2d, but got input of size: [16, 512]"]}]}]}